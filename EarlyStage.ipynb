{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# -----------Section 1: Importing Libraries-----------\n",
    "# ====================================================\n",
    "# 1.1. Data Manipulation, Statistics, and Feature Engineering\n",
    "# ====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import randint\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "# ====================================================\n",
    "# 1.2. Data Visualization\n",
    "# ====================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import ptitprince as pt\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1.3. Data Splitting, Model Building, and Hyperparameter Tuning\n",
    "# ============================\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB                   # Naive Bayes classifier\n",
    "from sklearn.linear_model import LogisticRegression          # Logistic regression\n",
    "from sklearn.ensemble import RandomForestClassifier # Ensemble methods\n",
    "from sklearn.svm import SVC                                   # Support vector classifier\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score         # Cross-validation techniques\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1.4. Model Evaluation and Interpretation\n",
    "# ============================\n",
    "import shap                          # SHAP (Shapley values) for model interpretability\n",
    "import lime                          # LIME (Local Interpretable Model-agnostic Explanations) for model interpretability\n",
    "import lime.lime_tabular            # LIME (Local Interpretable Model-agnostic Explanations) for model interpretability\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 2: Data Loading, Exploration------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 2.1 Load the dataset\n",
    "# ----------------------------\n",
    "\n",
    "esd_df = pd.read_csv(r\"https://github.com/EvanQLi/undergrad-diabetes-prediction-xai/blob/main/Early_Stage_Diabetes_Risk_Dataset.csv\")\n",
    "# ----------------------------\n",
    "# 2.2 Initial dataset inspection\n",
    "# ----------------------------\n",
    "#2.2.1 DataFrame Check\n",
    "print(\"Dataset row Indices:\")\n",
    "print(esd_df.index)\n",
    "print(\"Dataset Column Names:\")\n",
    "print(esd_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2.2 Data Overview\n",
    "esd_df.head()  # Display the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Info:\")\n",
    "print(esd_df.info())  # Display dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2.3 Descriptive Statistics\n",
    "esd_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 3: Data Cleaning------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 3.1 Missing Value Imputation\n",
    "# ----------------------------\n",
    "# 3.1.1 Missing Value Check\n",
    "print(\"\\nMissing Values:\")\n",
    "print(esd_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3.2 Duplicate Value Check\n",
    "# ----------------------------\n",
    "# 3.2.1 Duplicate Value Check\n",
    "print(f\"Number of duplicate rows: {esd_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.2 Drop Duplicate Rows\n",
    "esd_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(esd_df.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3.3 Outlier Detection using IQR\n",
    "# ----------------------------\n",
    "# Define the function to find outliers using IQR\n",
    "def find_outliers_IQR(data):\n",
    "\tQ1 = data.quantile(0.25)\n",
    "\tQ3 = data.quantile(0.75)\n",
    "\tIQR = Q3 - Q1\n",
    "\toutliers = (data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))\n",
    "\tlow_outliers = data < (Q1 - 1.5 * IQR)\n",
    "\thigh_outliers = data > (Q3 + 1.5 * IQR)\n",
    "\treturn outliers, low_outliers, high_outliers\n",
    "\n",
    "# 3.3.1 Outlier Detection\n",
    "# Outliers for 'Age' in esd_df\n",
    "print(\"\\n Outliers in 'Age'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(esd_df['Age'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(esd_df['Age'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(esd_df['Age'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 4: Exploratory Data Analysis------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 4.1 Target Variable Analysis\n",
    "# ----------------------------\n",
    "#4.1.1 Brief overview of target variable\n",
    "print(\"\\nDistribution of target variable:\")\n",
    "print(esd_df['class'].value_counts())\n",
    "\n",
    "# 4.1.2 Visualizing the Target Variable (Bar Chart with Percentages)\n",
    "# Compute class distribution\n",
    "class_counts = esd_df['class'].value_counts().sort_index()  # Ensure order: 0 (Negative), 1 (Positive)\n",
    "class_percentage = (class_counts / class_counts.sum()) * 100  # Convert to percentage\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=class_counts.index,\n",
    "    y=class_percentage.values,\n",
    "    palette=['#4682B4', '#FF6347']  # Blue for Negative, Red for Positive\n",
    ")\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for index, value in enumerate(class_percentage):\n",
    "    plt.text(index, value + 1, f'{value:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "# Customize chart aesthetics\n",
    "plt.title('Percentage of Diabetic vs. Non-Diabetic Cases', fontsize=14)\n",
    "plt.xlabel('Outcome', fontsize=12)\n",
    "plt.ylabel('Percentage (%)', fontsize=12)\n",
    "plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])  # Rename x-axis labels\n",
    "plt.ylim(0, 100)  # Set y-axis limits from 0 to 100%\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.2 Age Feature Analysis\n",
    "# ----------------------------\n",
    "#4.2.1 Age Distribution Analysis\n",
    "# Create age groups based on WHO age classification\n",
    "bins = [0, 44, 60, 75, 90, 100]  # Define age bins\n",
    "labels = ['Young Age', 'Middle Age', 'Elderly Age', 'Senile Age', 'Long-Livers']  # Define labels for bins\n",
    "esd_df['age_group'] = pd.cut(esd_df['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Verify the distribution of age groups\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "print(esd_df['age_group'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2.2 Age Distribution Across Different Age Groups\n",
    "# Raincloud plot\n",
    "import numpy\n",
    "# Patching the asscalar function to avoid error\n",
    "def patch_asscalar(a):\n",
    "    return a.item()\n",
    "\n",
    "setattr(numpy, \"asscalar\", patch_asscalar)\n",
    "\n",
    "\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "diabetic_color = \"#FF4500\"  # Bright orange-red\n",
    "non_diabetic_color = \"#1E90FF\"  # Bright deep blue\n",
    "\n",
    "# Create Raincloud plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "pt.half_violinplot(\n",
    "    data=esd_df,\n",
    "    x=\"Age\",\n",
    "    y=\"age_group\",\n",
    "    inner=None,\n",
    "    linewidth=1.5,\n",
    "    split=True,\n",
    "    palette=\"Set3\",\n",
    "    edgecolor='white',\n",
    "    dodge=True,\n",
    "    orient=\"h\",\n",
    ")\n",
    "\n",
    "# Add boxplot\n",
    "sns.boxplot(\n",
    "    data=esd_df,\n",
    "    x=\"Age\",\n",
    "    y=\"age_group\",\n",
    "    width=0.15,\n",
    "    zorder=10,\n",
    "    boxprops = {'facecolor':'none', \"zorder\":10},\n",
    "    whiskerprops = {'linewidth':2, \"zorder\":10}\n",
    "    )   \n",
    "\n",
    "# Add swarmplot\n",
    "sns.swarmplot(\n",
    "    data=esd_df,\n",
    "    x=\"Age\",\n",
    "    y=\"age_group\",\n",
    "    hue=\"class\",\n",
    "    size=4,\n",
    "    edgecolor='white',\n",
    "    linewidth=0.5,\n",
    "    palette=[diabetic_color, non_diabetic_color],\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "# Add Count Labels to Each Age Group\n",
    "age_group_counts = esd_df.groupby([\"age_group\", \"class\"]).size().unstack(fill_value=0)\n",
    "# ✅ Get rightmost x-limit for alignment\n",
    "x_max = plt.xlim()[1]  \n",
    "text_x = x_max + 5  # Move labels further right\n",
    "\n",
    "# ✅ Add Count Labels to Each Age Group\n",
    "for i, (age_group, row) in enumerate(age_group_counts.iterrows()):\n",
    "    non_diabetic_count = row.get(0, 0)  # Get count safely\n",
    "    diabetic_count = row.get(1, 0)  # Get count safely\n",
    "\n",
    "    plt.text(text_x, i + 0.15, f\"Non-Diabetic: {non_diabetic_count}\", fontsize=13,\n",
    "             fontweight=\"bold\", color=non_diabetic_color, ha=\"left\")\n",
    "\n",
    "    plt.text(text_x, i - 0.15, f\"Diabetic: {diabetic_count}\", fontsize=13,\n",
    "             fontweight=\"bold\", color=diabetic_color, ha=\"left\")\n",
    "\n",
    "# Improve plot readability\n",
    "plt.title(\"Age Distribution Across Different Age Groups\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Age\", fontsize=14)\n",
    "plt.ylabel(\"Age Group\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(title=\"Diabetic Status\", title_fontsize=\"13\", fontsize=\"12\", loc=\"upper right\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2.3 Drop the 'age_group' column\n",
    "esd_df.drop(columns=['age_group'], inplace=True)\n",
    "\n",
    "esd_df_copy = esd_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.3 Gender Feature Analysis\n",
    "# ----------------------------\n",
    "#4.3.1 Overview of Gender Distribution\n",
    "gender_counts = esd_df['Gender'].value_counts().sort_index()  # Ensure order: 1(Male), 0 (Female)\n",
    "gender_percentage = (gender_counts / gender_counts.sum()) * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nGender Distribution:\")\n",
    "print(gender_counts)\n",
    "print(\"\\nGender Percentage Distribution:\")\n",
    "print(gender_percentage.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3.2 Gender Distribution Bar Chart \n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=gender_counts.index,\n",
    "    y=gender_percentage.values,\n",
    "    palette=['#ff7f0e', '#1f77b4']  # Blue for Male, Orange for Female\n",
    ")\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for index, value in enumerate(gender_percentage):\n",
    "    plt.text(index, value + 1, f'{value:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize chart aesthetics\n",
    "plt.title('Gender Distribution Percentage in ESD', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Gender', fontsize=12)\n",
    "plt.ylabel('Percentage (%)', fontsize=12)\n",
    "plt.xticks([0, 1], ['Female', 'Male'])  # Ensure labels match encoded values\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# 3. Gender Distribution Across Diabetes Classes\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.countplot(\n",
    "    x='Gender', hue='class', data=esd_df, \n",
    "    palette=['#FF6347', '#4682B4'],  # Blue for Negative, Red for Positive\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add count labels on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f'{int(p.get_height())}',  # Convert float to integer for better readability\n",
    "        (p.get_x() + p.get_width() / 2., p.get_height()),  # Position label at the top of the bar\n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold', color='black'\n",
    "    )\n",
    "\n",
    "# Customize chart aesthetics\n",
    "plt.title('Gender Distribution by Diabetes Status', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Gender', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks([0, 1], ['Male', 'Female'])  # Ensure labels match dataset values\n",
    "plt.legend(title='Diabetes Status', labels=['Positive', 'Negative'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.4 Binary Features Analysis\n",
    "# ----------------------------\n",
    "#4.4.1 Data Encoding for Binary Features Analysis\n",
    "\n",
    "# Convert class labels to binary\n",
    "esd_df_copy['class'] = esd_df_copy['class'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "# Convert Categorical Features to Numerical via Label Encoding\n",
    "\n",
    "# Create LabelEncoder instance\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply to all binary categorical columns\n",
    "binary_cols = ['Gender', 'Polyuria', 'Polydipsia', 'sudden weight loss','weakness',\n",
    "               'Polyphagia','Genital thrush','visual blurring','Itching','Irritability',\n",
    "               'delayed healing','partial paresis','muscle stiffness','Alopecia',\n",
    "               'Obesity'\n",
    "               ]  \n",
    "for col in binary_cols:\n",
    "    esd_df_copy[col] = le.fit_transform(esd_df[col])\n",
    "\n",
    "# Check the transformed dataset\n",
    "print(esd_df_copy.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# #4.4.2 Compute Distribution Rate of Binary Features\n",
    "#------------------------\n",
    "# Extract binary features (excluding target variable)\n",
    "binary_features = [col for col in esd_df_copy.columns if esd_df_copy[col].nunique() == 2 and col != 'class'and col != 'Gender']\n",
    "\n",
    "# Compute proportion of \"Yes\" (1) and \"No\" (0) responses\n",
    "binary_summary = pd.DataFrame({\n",
    "    'Feature': binary_features,\n",
    "    'Yes (%)': [(esd_df_copy[col].mean()) * 100 for col in binary_features],  # Mean gives proportion of 1s\n",
    "    'No (%)': [(1 - esd_df_copy[col].mean()) * 100 for col in binary_features]\n",
    "})\n",
    "\n",
    "# Rank features by the highest \"Yes\" rate\n",
    "binary_summary = binary_summary.sort_values(by='Yes (%)', ascending=False)\n",
    "\n",
    "# Print ranked summary\n",
    "print(\"\\nBinary Feature Distribution Ranked by 'Yes' Rate:\")\n",
    "print(binary_summary.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# #4.4.3 Visualize Distribution Rate of Binary Features\n",
    "#------------------------\n",
    "# Convert data to long format for visualization\n",
    "binary_summary_melted = binary_summary.melt(id_vars=['Feature'], var_name='Response', value_name='Percentage')\n",
    "# Set figure size\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Create stacked horizontal bar chart\n",
    "binary_summary.set_index(\"Feature\")[[\"Yes (%)\", \"No (%)\"]].plot(\n",
    "    kind=\"barh\", stacked=True, color=['#b2d7ca', '#3884a5'], edgecolor=\"black\", figsize=(16, 10),fontsize=12\n",
    ")\n",
    "\n",
    "# Improve readability\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Distribution Rate of Binary Features\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "# Add percentage labels inside bars\n",
    "for i, (yes, no) in enumerate(zip(binary_summary[\"Yes (%)\"], binary_summary[\"No (%)\"])):\n",
    "    plt.text(yes / 2, i - 0.15, f\"{yes:.1f}%\", fontsize=12, color=\"black\", ha=\"center\", fontweight=\"bold\" )  # Yes inside bar\n",
    "    plt.text(yes + (no / 2), i - 0.15, f\"{no:.1f}%\", fontsize=12, color=\"black\", ha=\"center\", fontweight=\"bold\" )  # No inside bar\n",
    "\n",
    "# Adjust grid\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "# Add legend outside the plot\n",
    "plt.legend([\"Yes (%)\", \"No (%)\"], title=\"Response\", fontsize=12, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "#4.4.4 Visualize Binary Features by Diabetes Status\n",
    "#------------------------\n",
    "# Automatically determine grid size based on number of binary features\n",
    "num_features = len(binary_features)\n",
    "num_cols = 3  # Fixed number of columns\n",
    "num_rows = math.ceil(num_features / num_cols)  # Compute required rows\n",
    "\n",
    "# Set up a flexible grid of subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, num_rows * 3))  \n",
    "axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "\n",
    "# Loop through each binary feature and plot its distribution across diabetes classes\n",
    "for i, col in enumerate(binary_features):\n",
    "    ax = sns.countplot(\n",
    "        x=col, hue='class', data=esd_df_copy, order=[0, 1],  # Ensure order: 1 (Yes), 0 (No)\n",
    "        palette=['#1E90FF', '#FF4500'], edgecolor='black', ax=axes[i]\n",
    "    )\n",
    "    # ✅ Rename X-axis labels from 0 → \"No\" and 1 → \"Yes\"\n",
    "    ax.set_xticklabels([\"No\", \"Yes\"])\n",
    "    # Get the maximum y-axis value for scaling\n",
    "    max_count = max([p.get_height() for p in ax.patches if p.get_height() > 0])\n",
    "\n",
    "    # Set y-axis limit slightly above max count to prevent labels from overlapping\n",
    "    ax.set_ylim(0, max_count * 1.15)  \n",
    "\n",
    "    # Add count labels dynamically positioned within the plot\n",
    "    for p in ax.patches:\n",
    "        if p.get_height() > 0:  # Skip labels for bars with zero height\n",
    "            ax.annotate(\n",
    "                f'{int(p.get_height())}',  \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height() + (max_count * 0.02)),  \n",
    "                ha='center', fontsize=10, fontweight='bold', color='black'\n",
    "            )\n",
    "\n",
    "    # Customize titles and labels\n",
    "    ax.set_title(f'{col} by Diabetes Status', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(col, fontsize=13)\n",
    "    ax.set_ylabel('Count', fontsize=13)\n",
    "\n",
    "    # Remove individual legends to avoid duplicates\n",
    "    if i != len(binary_features):  \n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Remove any extra blank subplots that were created\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Add a single legend on the right side of the figure\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, ['Negative', 'Positive'], title='Diabetes Status', loc='center right', fontsize=14)\n",
    "\n",
    "# Adjust layout for better readability\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Leave space on the right for the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# 4.4.4 Visualize Binary Features by Diabetes Status (Using Probabilities with Percentage Labels)\n",
    "#------------------------\n",
    "\n",
    "# Automatically determine grid size based on number of binary features\n",
    "num_features = len(binary_features)\n",
    "num_cols = 3  # Fixed number of columns\n",
    "num_rows = math.ceil(num_features / num_cols)  # Compute required rows\n",
    "\n",
    "# Set up a flexible grid of subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, num_rows * 3))\n",
    "axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "# Loop through each binary feature and plot its probabilities across diabetes classes\n",
    "for i, col in enumerate(binary_features):\n",
    "    # Calculate probabilities for each class\n",
    "    probabilities = (\n",
    "        esd_df_copy.groupby(['class', col])\n",
    "        .size()\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda x: x / x.sum())\n",
    "        .unstack()\n",
    "    )\n",
    "    \n",
    "    # Plot the probabilities\n",
    "    ax = probabilities.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        color=['#1E90FF', '#FF4500'],  # Blue for \"No\", Red for \"Yes\"\n",
    "        edgecolor='black',\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    \n",
    "    # Add percentage labels inside the bars\n",
    "    for p in ax.patches:\n",
    "        width, height = p.get_width(), p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        if height > 0:  # Avoid labeling bars with zero height\n",
    "            ax.text(\n",
    "                x + width / 2, y + height / 2,\n",
    "                f'{height * 100:.1f}%',  # Convert to percentage\n",
    "                ha='center', va='center', fontsize=10, color='white', fontweight='bold'\n",
    "            )\n",
    "    \n",
    "    # Customize titles and labels\n",
    "    axes[i].set_title(f'{col} by Diabetes Status (Probabilities)', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Diabetes Status', fontsize=13)\n",
    "    axes[i].set_ylabel('Probability', fontsize=13)\n",
    "    axes[i].set_xticklabels(['Negative', 'Positive'], rotation=0)  # Rename x-axis labels\n",
    "\n",
    "  # Remove individual legends to avoid duplicates\n",
    "    if i != len(binary_features):  \n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Remove any extra blank subplots that were created\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Add a single legend on the right side of the figure\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, ['Negative', 'Positive'], title='Diabetes Status', loc='lower right', bbox_to_anchor=(0.9,0.1),fontsize=14)\n",
    "\n",
    "# Adjust layout for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.5 Correlation Analysis\n",
    "# ----------------------------\n",
    "#4.5.1 Heatmap of correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(esd_df_copy.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Top correlated features with Class\n",
    "correlations = esd_df_copy.corr()['class'].drop('class').sort_values(ascending=False)\n",
    "print(\"Top Correlated Features with Class:\")\n",
    "print(correlations)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create bar plot\n",
    "sns.barplot(x=correlations.values, y=correlations.index, dodge=False, palette='Set2')\n",
    "\n",
    "# Add count labels on each bar\n",
    "for i, v in enumerate(correlations):\n",
    "    plt.text(v, i, f\"{v:.2f}\", color='black', va='center', fontsize=12)\n",
    "\n",
    "# Improve readability\n",
    "plt.xlabel(\"Correlation Coefficient\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title('Feature Correlations with Class', fontsize=16, fontweight=\"bold\")\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 5: Feature Engineering------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 5.1 Data Encoding\n",
    "# ----------------------------\n",
    "# Convert class labels to binary\n",
    "esd_df['class'] = esd_df['class'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "# Convert Categorical Features to Numerical via Label Encoding\n",
    "\n",
    "# Create LabelEncoder instance\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply to all binary categorical columns\n",
    "binary_cols = ['Gender', 'Polyuria', 'Polydipsia', 'sudden weight loss','weakness',\n",
    "               'Polyphagia','Genital thrush','visual blurring','Itching','Irritability',\n",
    "               'delayed healing','partial paresis','muscle stiffness','Alopecia',\n",
    "               'Obesity'\n",
    "               ]  \n",
    "for col in binary_cols:\n",
    "    esd_df[col] = le.fit_transform(esd_df[col])\n",
    "\n",
    "# Check the transformed dataset\n",
    "print(esd_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5.2 Data Splitting\n",
    "# ----------------------------\n",
    "\n",
    "#5.2.1 Split the dataset into features (X) and target variable (y)\n",
    "X = esd_df.drop(columns=['class'])\n",
    "y = esd_df['class']\n",
    "\n",
    "# Display the shapes of X and y to verify the split\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2.2 Split the dataset into training and testing sets(7:3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Validate class distribution in train and test splits\n",
    "print(\"Training class distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Testing class distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5.3 Data SMOTE\n",
    "# ----------------------------\n",
    "#not to do, although the dataset is imbalanced, \n",
    "#keep the original dataset could be better for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5.4 Data Scaling\n",
    "# ----------------------------\n",
    "# Create StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Transform the training and testing data for models not tree-based \n",
    "X_train_scaled[['Age']]= scaler.fit_transform(X_train_scaled[['Age']])\n",
    "X_test_scaled[['Age']] = scaler.transform(X_test_scaled[['Age']])\n",
    "\n",
    "# Display the shape of the splits\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 6: Model Building and Hyperparameter Tuning-----------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 6.1 Naive Bayes\n",
    "# ----------------------------\n",
    "# 6.1.1 Model Building and Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Initialize the RandomizedSearchCV object\n",
    "nb_search = GridSearchCV(\n",
    "    estimator=nb, \n",
    "    param_grid={\"var_smoothing\": np.logspace(0,-2, num=100)},  # Use GridSearchCV \n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=1  # Display progress\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "nb_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "nb_best_params = nb_search.best_params_\n",
    "print(\"Best Parameters for Naive Bayes:\", nb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.1.2 Model Evaluation\n",
    "\n",
    "# Initialize the Naive Bayes classifier with the best parameters\n",
    "nb_best_model = GaussianNB(var_smoothing=nb_best_params['var_smoothing'])\n",
    "nb_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_nb = nb_best_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "nb_acc = accuracy_score(y_test, y_pred_nb)\n",
    "nb_conf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
    "nb_roc_auc = roc_auc_score(y_test, y_pred_nb)\n",
    "nb_f1 = f1_score(y_test, y_pred_nb)\n",
    "nb_precision = precision_score(y_test, y_pred_nb)\n",
    "nb_recall = recall_score(y_test, y_pred_nb)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Naive Bayes Accuracy: {nb_acc:.2f}\")\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(nb_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "print(f\"ROC-AUC Score: {nb_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {nb_f1:.2f}\")\n",
    "print(f\"Precision: {nb_precision:.2f}\")\n",
    "print(f\"Recall: {nb_recall:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(nb_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.2 Logistic Regression\n",
    "# ----------------------------\n",
    "# 6.2.1 Model Building and Hyperparameter Tuning using GridSearchCV\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "lr_params = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # Regularization strengths\n",
    "    \"penalty\": [\"l2\", \"l1\"],  # Regularization penalties\n",
    "    \"solver\": [\"liblinear\", \"saga\"],  # Solvers compatible with both l1 and l2 penalties\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=lr_params,\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    verbose=1,  # Display progress during fitting\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model with hyperparameter tuning\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "lr_best_params = lr_grid.best_params_\n",
    "print(\"Best Parameters for Logistic Regression:\", lr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2.2 Model Evaluation\n",
    "\n",
    "# Initialize the Logistic Regression classifier with the best parameters\n",
    "lr_best_model = LogisticRegression(**lr_best_params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_lr = lr_best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "lr_roc_auc = roc_auc_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Logistic Regression Accuracy: {lr_acc:.2f}\")\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(lr_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(f\"ROC-AUC Score: {lr_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {lr_f1:.2f}\")\n",
    "print(f\"Precision: {lr_precision:.2f}\")\n",
    "print(f\"Recall: {lr_recall:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(lr_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.3 Random Forest\n",
    "# ----------------------------\n",
    "\n",
    "# 6.3.1 Model Building and Hyperparameter Tuning\n",
    "\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier()  # Set random_state for reproducibility\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],  # Number of trees in the forest\n",
    "    \"criterion\": [\"gini\", \"entropy\"],  # Split criterion\n",
    "    \"max_depth\": [10, 20, 30, 40, 50, None],  # Maximum depth of the tree\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split an internal node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_params,\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    verbose=1,  # Display progress\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to find the best hyperparameters\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "rf_best_params = rf_grid.best_params_\n",
    "print(\"Best Parameters for Random Forest:\", rf_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6.3.2 Model Evaluation\n",
    "\n",
    "# Initialize the Random Forest classifier with the best hyperparameters\n",
    "rf_best_model = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "# Fit the final model on the training data\n",
    "rf_best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict the outcomes for the test set\n",
    "rf_pred = rf_best_model.predict(X_test)\n",
    "rf_pred_prob = rf_best_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_pred)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_pred_prob)\n",
    "rf_f1 = f1_score(y_test, rf_pred)\n",
    "rf_precision = precision_score(y_test, rf_pred)\n",
    "rf_recall = recall_score(y_test, rf_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.3f}\")\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(rf_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "print(f\"Random Forest ROC-AUC Score: {rf_roc_auc:.3f}\")\n",
    "print(f\"F1-Score: {rf_f1:.3f}\")\n",
    "print(f\"Precision: {rf_precision:.3f}\")\n",
    "print(f\"Recall: {rf_recall:.3f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.4 Support Vector Machine (SVM)\n",
    "# ----------------------------\n",
    "\n",
    "# 6.4.1 Model Building and Hyperparameter Tuning\n",
    "\n",
    "# Initialize the Support Vector Machine classifier\n",
    "\n",
    "# svm = SVC()  # Set random_state for reproducibility\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "# svm_params = {\n",
    "#     \"C\": [1, 10],  # Regularization parameter\n",
    "#     \"kernel\": [\"linear\", \"rbf\"],  # Kernel type\n",
    "#     \"gamma\": [\"scale\", \"auto\"],  # Kernel coefficient\n",
    "# }   \n",
    "#     # Running to slow with parameters below(need to be commented out),\n",
    "#     #\"C\": [0.1, 1, 10, 100],  # Wider range of regularization strengths\n",
    "#     # \"kernel\": [\"linear\", \"rbf\", \"poly\"],  # Adding polynomial kernel for flexibility\n",
    "#     # \"degree\": [2, 3],  # Polynomial kernel degree (only applicable for \"poly\")\n",
    "#     # \"gamma\": [\"scale\", \"auto\"]  # Kernel coefficient\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "# svm_grid = GridSearchCV(\n",
    "#     estimator=svm,\n",
    "#     param_grid=svm_params,\n",
    "#     scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "#     cv=10,  # 10-fold cross-validation\n",
    "#     verbose=1,  # Display progress\n",
    "#     n_jobs=-1  # Use all available processors\n",
    "# )\n",
    "\n",
    "# Fit the GridSearchCV object to find the best hyperparameters\n",
    "# svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "# svm_best_params = svm_grid.best_params_\n",
    "# print(\"Best Parameters for Support Vector Machine:\", svm_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4.2 Model Evaluation\n",
    "\n",
    "# Initialize the SVM classifier with the best hyperparameters\n",
    "# svm_best_model = SVC(**svm_best_params)\n",
    "\n",
    "# Fit the final model on the training data\n",
    "# svm_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the outcomes for the test set\n",
    "# svm_pred = svm_best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "# svm_acc = accuracy_score(y_test, svm_pred)\n",
    "# svm_conf_matrix = confusion_matrix(y_test, svm_pred)\n",
    "# svm_roc_auc = roc_auc_score(y_test, svm_pred)\n",
    "# svm_f1 = f1_score(y_test, svm_pred)\n",
    "# svm_precision = precision_score(y_test, svm_pred)\n",
    "# svm_recall = recall_score(y_test, svm_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "# print(f\"SVM Accuracy: {svm_acc:.3f}\")\n",
    "# print(\"SVM Confusion Matrix:\")\n",
    "# print(svm_conf_matrix)\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, svm_pred))\n",
    "# print(f\"SVM ROC-AUC Score: {svm_roc_auc:.3f}\")\n",
    "# print(f\"F1-Score: {svm_f1:.3f}\")\n",
    "# print(f\"Precision: {svm_precision:.3f}\")\n",
    "# print(f\"Recall: {svm_recall:.3f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.heatmap(svm_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "# plt.title(\"SVM Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.5 Extreme Gradient Boosting (XGBoost)\n",
    "# ----------------------------\n",
    "\n",
    "# 6.5.1 Model Building and Hyperparameter Tuning\n",
    "\n",
    "# Initialize the Gradient Boosting (XGBoost) classifier\n",
    "gb = XGBClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.5],  # Step size shrinkage\n",
    "    'max_depth': [2, 3, 4, 5],  # Maximum depth of trees\n",
    "    'subsample': [0.5, 0.8, 1.0],  # Fraction of samples for training\n",
    "    'colsample_bytree': [0.1, 0.2, 0.3, 0.5],  # Fraction of features per tree\n",
    "    'colsample_bylevel': [0.1, 0.2, 0.3, 0.5],  # Fraction of features per level\n",
    "    'min_child_weight': [1, 3, 5, 7],  # Minimum sum of weights of child nodes\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "gb_grid = GridSearchCV(\n",
    "    estimator=gb,\n",
    "    param_grid=gb_params,\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to find the best hyperparameters\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters and best estimator\n",
    "gb_best_params = gb_grid.best_params_\n",
    "gb_model = gb_grid.best_estimator_\n",
    "print(\"Best Parameters for Gradient Boosting (XGBoost):\", gb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6.5.2 Model Evaluation\n",
    "\n",
    "# Initialize the Gradient Boosting (XGBoost) classifier with the best hyperparameters\n",
    "gb_best_model = XGBClassifier(**gb_best_params)\n",
    "\n",
    "# Fit the final model on the training data\n",
    "gb_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the test set\n",
    "gb_pred = gb_best_model.predict(X_test)\n",
    "gb_pred_prob = gb_best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "gb_acc = accuracy_score(y_test, gb_pred)\n",
    "gb_conf_matrix = confusion_matrix(y_test, gb_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, gb_pred_prob)\n",
    "gb_f1 = f1_score(y_test, gb_pred)\n",
    "gb_precision = precision_score(y_test, gb_pred)\n",
    "gb_recall = recall_score(y_test, gb_pred)\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "sensitivity = gb_conf_matrix[1, 1] / (gb_conf_matrix[1, 1] + gb_conf_matrix[1, 0])\n",
    "specificity = gb_conf_matrix[0, 0] / (gb_conf_matrix[0, 0] + gb_conf_matrix[0, 1])\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Gradient Boosting Accuracy: {gb_acc:.2f}\")\n",
    "print(\"Gradient Boosting Confusion Matrix:\")\n",
    "print(gb_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, gb_pred))\n",
    "print(f\"ROC-AUC Score: {gb_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {gb_f1:.2f}\")\n",
    "print(f\"Precision: {gb_precision:.2f}\")\n",
    "print(f\"Recall: {gb_recall:.2f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(gb_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Gradient Boosting Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.6 Stacking Classifier\n",
    "# ----------------------------\n",
    "%pip install mlxtend\n",
    "\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "# Initialize the base classifiers\n",
    "base_classifiers = [\n",
    "    LogisticRegression(**lr_best_params),\n",
    "    RandomForestClassifier(**rf_best_params),\n",
    "    XGBClassifier(**gb_best_params)\n",
    "]\n",
    "\n",
    "# Define the meta-classifier\n",
    "meta_classifier = RandomForestClassifier()\n",
    "\n",
    "# Initialize the StackingCV classifier\n",
    "stacking = StackingCVClassifier(\n",
    "    classifiers=base_classifiers,\n",
    "    meta_classifier=meta_classifier,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    stratify=True,  # Use stratified folds\n",
    "    shuffle=True,  # Shuffle the data\n",
    "    n_jobs=-1,  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the StackingCV classifier\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable\n",
    "y_stacking_pred = stacking.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "stacking_acc = accuracy_score(y_test, y_stacking_pred)\n",
    "stacking_conf_matrix = confusion_matrix(y_test, y_stacking_pred)\n",
    "stacking_roc_auc = roc_auc_score(y_test, y_stacking_pred)\n",
    "stacking_f1 = f1_score(y_test, y_stacking_pred)\n",
    "stacking_precision = precision_score(y_test, y_stacking_pred)\n",
    "stacking_recall = recall_score(y_test, y_stacking_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Stacking Classifier Accuracy: {stacking_acc:.2f}\")\n",
    "print(\"Stacking Classifier Confusion Matrix:\")\n",
    "print(stacking_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_stacking_pred))\n",
    "print(f\"ROC-AUC Score: {stacking_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {stacking_f1:.2f}\")\n",
    "print(f\"Precision: {stacking_precision:.2f}\")\n",
    "print(f\"Recall: {stacking_recall:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(stacking_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Stacking Classifier Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 7: Model Performance Comparison------------\n",
    "# ========================================================================\n",
    "\n",
    "# 7.1 Model Performance Metrics \n",
    "\n",
    "# Create a DataFrame to display the model performance metrics\n",
    "\n",
    "# Initialize data for the model performance DataFrame\n",
    "data = {\n",
    "    \"Model\": [\"NB\", \"LR\", \"RF\", \"XGB\", \"Stacking\"],\n",
    "    \"Accuracy\": [round(nb_acc, 3), round(lr_acc, 3), round(rf_acc, 3), round(gb_acc, 3), round(stacking_acc, 3)],\n",
    "    \"ROC-AUC\": [round(nb_roc_auc, 3), round(lr_roc_auc, 3), round(rf_roc_auc, 3), round(gb_roc_auc, 3), round(stacking_roc_auc, 3)],\n",
    "    \"F1-Score\": [round(nb_f1, 3), round(lr_f1, 3), round(rf_f1, 3), round(gb_f1, 3), round(stacking_f1, 3)],\n",
    "    \"Precision\": [round(nb_precision, 3), round(lr_precision, 3), round(rf_precision, 3), round(gb_precision, 3), round(stacking_precision, 3)],\n",
    "    \"Recall\": [round(nb_recall, 3), round(lr_recall, 3), round(rf_recall, 3), round(gb_recall, 3), round(stacking_recall, 3)]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "model_comparison = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "\n",
    "# 7.2 Visualizing Model  Performance\n",
    "\n",
    "\n",
    "# 7.2.1 Create a Bar Plot for Model Comparison\n",
    "\n",
    "# Convert the data to long format\n",
    "model_comparison_long = pd.melt(model_comparison, id_vars=[\"Model\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# customed color\n",
    "colors = [\"#e2edc9\", \"#a4d3b7\", \"#65c4b9\", \"#26a7c8\", \"#607bbc\"]\n",
    "# Create a bar plot to compare the model performance metrics\n",
    "sns.barplot(x=\"Model\", y=\"Value\", hue=\"Metric\", data=model_comparison_long, palette=colors)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Model Performance Comparison\", fontsize=16)\n",
    "plt.xlabel(\"Model\", fontsize=16)\n",
    "plt.ylabel(\"Performance Metric\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=12, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 8: Feature Interpretability------------\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8.1 Importance ranking of features in the model built-in\n",
    "# ----------------------------\n",
    "# 8.1.1 Random Forest Feature Importance\n",
    "# Extract feature importances from the Random Forest model\n",
    "rf_feature_importance = rf_best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importance values\n",
    "rf_fi_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": rf_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the DataFrame based on feature importance\n",
    "rf_fi_df = rf_fi_df.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the feature importances\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(rf_fi_df)\n",
    "\n",
    "\n",
    "\n",
    "#plot the feature importance\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=rf_fi_df, palette=\"viridis\")\n",
    "\n",
    "# Add percentage labels on each bar\n",
    "for index, value in enumerate(rf_fi_df[\"Importance\"]):\n",
    "    plt.text(value, index, f'{value:.2%}', color='black', va=\"center\", fontsize=6)\n",
    "\n",
    "plt.title(\"Random Forest Feature Importance\", fontsize=16)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2 Gradient Boosting Feature Importance\n",
    "\n",
    "# Extract feature importances from the Gradient Boosting model\n",
    "gb_feature_importance = gb_best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importance values\n",
    "gb_fi_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": gb_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the DataFrame based on feature importance\n",
    "gb_fi_df = gb_fi_df.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the feature importances\n",
    "print(\"\\nGradient Boosting Feature Importance:\")\n",
    "print(gb_fi_df)\n",
    "\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=gb_fi_df, palette=\"viridis\")\n",
    "\n",
    "# Add percentage labels on each bar\n",
    "for index, value in enumerate(gb_fi_df[\"Importance\"]):\n",
    "    plt.text(value, index, f'{value:.2%}', color='black', va=\"center\", fontsize=6)\n",
    "\n",
    "plt.title(\"Gradient Boosting Feature Importance\", fontsize=16)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8.2 Importance ranking of features by SHAP values\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# 8.2.1 SHAP Analysis for Random Forest\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize the SHAP explainer for the best Random Forest model\n",
    "rf_explainer = shap.Explainer(rf_best_model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "rf_shap_values = rf_explainer.shap_values(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Visualize the SHAP values\n",
    "shap.summary_plot(rf_shap_values[:,:,1], X_test,  feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.2.2 SHAP Analysis for Gradient Boosting\n",
    "# Create object that can calculate shap values for Gradient Boosting model\n",
    "explainer_gb = shap.TreeExplainer(gb_best_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of X_test rather than a single row, to have more data for plot.\n",
    "shap_values_gb = explainer_gb.shap_values(X_test)\n",
    "\n",
    "# Make plot for Gradient Boosting model\n",
    "shap.summary_plot(shap_values_gb, X_test, feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8.3 Local Interpretability using LIME\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# 8.3.1 True Positive Case in ESD\n",
    "# ----------------------------\n",
    "# Find the index of a true positive case in random forest, xgboost, and stacking\n",
    "true_positive_idx = y_test[(y_test == 1) & (rf_pred == 1) & (gb_pred ==1) & (y_stacking_pred == 1)].index[0]\n",
    "\n",
    "# Extract the features for the true positive case\n",
    "ESD_X_true_positive = X_test.loc[[true_positive_idx]]\n",
    "\n",
    "print(\"\\nTrue Positive Case:\")\n",
    "print(ESD_X_true_positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lime explainer for the true positive case via random forest\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true positive case\n",
    "exp = explainer.explain_instance(ESD_X_true_positive.values[0], rf_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true positive case via XGBoost\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true positive case\n",
    "exp = explainer.explain_instance(ESD_X_true_positive.values[0], gb_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true positive case via stacking classifier\n",
    "\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true positive case\n",
    "exp = explainer.explain_instance(ESD_X_true_positive.values[0], stacking.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8.3.2 True Negative Case in ESD\n",
    "# ----------------------------\n",
    "# Find the index of a true negative case in random forest, xgboost, and stacking\n",
    "true_negative_idx = y_test[(y_test == 0) & (rf_pred == 0) & (gb_pred == 0) & (y_stacking_pred == 0)].index[0]\n",
    "\n",
    "# Extract the features for the true negative case\n",
    "ESD_X_true_negative = X_test.loc[[true_negative_idx]]\n",
    "print(\"\\nTrue Negative Case:\")\n",
    "print(ESD_X_true_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true negative case via random forest\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true negative case\n",
    "exp = explainer.explain_instance(ESD_X_true_negative.values[0], rf_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true negative case via XGBoost\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true negative case\n",
    "exp = explainer.explain_instance(ESD_X_true_negative.values[0], gb_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true negative case via Stacking Classifier\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true negative case\n",
    "exp = explainer.explain_instance(ESD_X_true_negative.values[0], stacking.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
