{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# -----------Section 1: Importing Libraries-----------\n",
    "# ====================================================\n",
    "# 1.1. Data Manipulation, Statistics, and Feature Engineering\n",
    "# ====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import randint\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "# ====================================================\n",
    "# 1.2. Data Visualization\n",
    "# ====================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "import ptitprince as pt\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1.3. Data Splitting, Model Building, and Hyperparameter Tuning\n",
    "# ============================\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB                   # Naive Bayes classifier\n",
    "from sklearn.linear_model import LogisticRegression          # Logistic regression\n",
    "from sklearn.ensemble import RandomForestClassifier # Ensemble methods\n",
    "from sklearn.svm import SVC                                   # Support vector classifier\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score         # Cross-validation techniques\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1.4. Model Evaluation and Interpretation\n",
    "# ============================\n",
    "import shap                          # SHAP (Shapley values) for model interpretability\n",
    "import lime                          # LIME (Local Interpretable Model-agnostic Explanations) for model interpretability\n",
    "import lime.lime_tabular            # LIME (Local Interpretable Model-agnostic Explanations) for model interpretability\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 2: Data Loading, Exploration, and Cleaning------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 2.1 Load the dataset\n",
    "# ----------------------------\n",
    "\n",
    "pima_df= pd.read_csv(\"F:/University/大四上/MANG3099 Final Project/Dataset/diabetes.csv\")\n",
    "# ----------------------------\n",
    "# 2.2 Initial dataset inspection\n",
    "# ----------------------------\n",
    "#2.2.1 DataFrame Check\n",
    "print(\"Dataset row Indices:\")\n",
    "print(pima_df.index)\n",
    "print(\"Dataset Column Names:\")\n",
    "print(pima_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2.2 Data Overview\n",
    "pima_df.head()  # Display the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Info:\")\n",
    "print(pima_df.info())  # Display dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2.3 Descriptive Statistics\n",
    "pima_df.describe().T .round(2)  # Display descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 3: Data Cleaning------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 3.1 Missing Value Imputation(On these columns, a value of zero does not make sense and thus indicates missing value.)\n",
    "# ----------------------------\n",
    "# 3.1.1 Missing Value Check\n",
    "print(\"\\nMissing Values:\")\n",
    "# Replace zeros with NaN in specific columns\n",
    "pima_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = pima_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.nan)\n",
    "print( pima_df.isnull().sum())\n",
    "\n",
    "# 3.1.2 Missing Value Imputation\n",
    "# Remove rows with missing values in Glucose, BloodPressure and BMI\n",
    "pima_df.dropna(subset=['Glucose','BloodPressure','BMI'], inplace=True)\n",
    "\n",
    "# Impute missing values in SkinThickness and Insulin with median values\n",
    "pima_df['SkinThickness'].fillna(pima_df['SkinThickness'].median(), inplace=True)\n",
    "\n",
    "pima_df['Insulin'].fillna(pima_df['Insulin'].median(), inplace=True)\n",
    "\n",
    "print(\"\\n Missing Values after Cleaning \\n\\n\", pima_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3.2 Duplicate Value Check\n",
    "# ----------------------------\n",
    "# 3.2.1 Duplicate Value Check\n",
    "print(\"\\nDuplicate Values:\")\n",
    "print(pima_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3.3 Outlier Detection using IQR\n",
    "# ----------------------------\n",
    "\n",
    "#3.3.1 Function to detect outliers in every feature using IQR\n",
    "def find_outliers_IQR(pima_df):\n",
    "    Q1 = pima_df.quantile(0.25)\n",
    "    Q3 = pima_df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    alloutliers = pima_df.apply(lambda x: (x < (Q1 - 1.5 * IQR)) | (x > (Q3 + 1.5 * IQR)))\n",
    "    lowoutliers = pima_df.apply(lambda x: x < (Q1 - 1.5 * IQR))\n",
    "    highoutliers = pima_df.apply(lambda x: x > (Q3 + 1.5 * IQR))\n",
    "    return alloutliers, lowoutliers, highoutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3.2 Detect outliers in every feature using IQR\n",
    "# Outliers for 'Pregnancies' in pima_df\n",
    "print(\"\\n Outliers in 'Pregnancies'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['Pregnancies'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['Pregnancies'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['Pregnancies'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'Glucose' in pima_df\n",
    "print(\"\\n Outliers in 'Glucose'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['Glucose'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['Glucose'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['Glucose'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'SkinThickness' in pima_df\n",
    "print(\"\\n Outliers in 'SkinThickness'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['SkinThickness'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['SkinThickness'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['SkinThickness'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'Insulin' in pima_df\n",
    "print(\"\\n Outliers in 'Insulin'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['Insulin'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['Insulin'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['Insulin'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'DiabetesPedigreeFunction' in pima_df\n",
    "print(\"\\n Outliers in 'DiabetesPedigreeFunction'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['DiabetesPedigreeFunction'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['DiabetesPedigreeFunction'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['DiabetesPedigreeFunction'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'Blood Pressure' in pima_df\n",
    "print(\"\\n Outliers in 'Blood Pressure'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['BloodPressure'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['BloodPressure'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['BloodPressure'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'BMI' in pima_df\n",
    "print(\"\\n Outliers in 'BMI'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['BMI'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['BMI'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['BMI'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for 'Age' in pima_df\n",
    "print(\"\\n Outliers in 'Age'\")\n",
    "alloutliers, lowoutliers, highoutliers = find_outliers_IQR(pima_df['Age'])\n",
    "print(\"number of outliers: \" + str(alloutliers.sum()))\n",
    "print(\"max outlier value: \" + str(pima_df['Age'][alloutliers].max()))\n",
    "print(\"min outlier value: \" + str(pima_df['Age'][alloutliers].min()))\n",
    "print(\"number of low outliers: \" + str(lowoutliers.sum()))\n",
    "print(\"number of high outliers: \" + str(highoutliers.sum()))\n",
    "alloutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dataset after Data Cleaning\n",
    "pima_df.describe().T.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# ----------Section 4: Exploratory Data Analysis------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 4.1 Target Variable Analysis\n",
    "# ----------------------------\n",
    "#4.1.1 Brief overview of target variable\n",
    "print(\"\\nDistribution of target variable:\")\n",
    "print(pima_df['Outcome'].value_counts())\n",
    "\n",
    "# 4.1.2 Visualizing the Target Variable (Bar Chart with Percentages)\n",
    "# Calculate percentage distribution of the Outcome variable\n",
    "outcome_counts = pima_df['Outcome'].value_counts()\n",
    "outcome_percentage = (outcome_counts / outcome_counts.sum()) * 100\n",
    "\n",
    "# Create figure for bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Bar chart displaying percentage of positive vs. negative diabetes patients\n",
    "sns.barplot(\n",
    "    x=outcome_percentage.index,\n",
    "    y=outcome_percentage.values,\n",
    "    palette=['#4682B4', '#FF6347']  # Blue for Negative, Red for Positive\n",
    ")\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for index, value in enumerate(outcome_percentage):\n",
    "    plt.text(index, value + 1, f'{value:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "# Customize chart aesthetics\n",
    "plt.title('Percentage of Diabetic vs. Non-Diabetic Cases', fontsize=14)\n",
    "plt.xlabel('Outcome', fontsize=12)\n",
    "plt.ylabel('Percentage (%)', fontsize=12)\n",
    "plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])  # Rename x-axis labels\n",
    "plt.ylim(0, 100)  # Ensure y-axis represents percentage (0-100)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.2 Age Feature Analysis\n",
    "# ----------------------------\n",
    "#4.2.1 Age Distribution Analysis\n",
    "# Create age groups based on WHO age classification\n",
    "bins = [0, 44, 60, 75, 90, 100]  # Define age bins\n",
    "labels = ['Young Age', 'Middle Age', 'Elderly Age', 'Senile Age', 'Long-Livers']  # Define labels for bins\n",
    "pima_df['age_group'] = pd.cut(pima_df['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Verify the distribution of age groups\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "print(pima_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2.2 Age Distribution Across Different Age Groups\n",
    "import numpy\n",
    "# Patching the asscalar function to avoid error\n",
    "def patch_asscalar(a):\n",
    "    return a.item()\n",
    "\n",
    "setattr(numpy, \"asscalar\", patch_asscalar)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Define colors for diabetic and non-diabetic groups\n",
    "diabetic_color = \"#FF4500\"  # Bright orange-red\n",
    "non_diabetic_color = \"#1E90FF\"  # Bright deep blue\n",
    "\n",
    "# Calculate the total count for each age group\n",
    "age_group_totals = pima_df['age_group'].value_counts().sort_index()\n",
    "\n",
    "# Create a new DataFrame to store the percentage values\n",
    "age_group_percentages = pima_df.groupby(['age_group', 'Outcome']).size().unstack(fill_value=0).div(age_group_totals, axis=0) * 100\n",
    "\n",
    "# Create stacked horizontal bar chart with percentage values\n",
    "ax = age_group_percentages.plot(kind='barh', stacked=True, color=[non_diabetic_color, diabetic_color], figsize=(14, 8))\n",
    "\n",
    "# Add Count Labels to Each Age Group\n",
    "age_group_counts = pima_df.groupby([\"age_group\", 'Outcome']).size().unstack(fill_value=0)\n",
    "\n",
    "# Add Percentage Labels on Each Bar\n",
    "for i, (age_group, row) in enumerate(age_group_percentages.iterrows()):\n",
    "    non_diabetic_percentage = row.get(0, 0)  # Get percentage safely\n",
    "    diabetic_percentage = row.get(1, 0)  # Get percentage safely\n",
    "\n",
    "    if non_diabetic_percentage > 0:\n",
    "        ax.text(non_diabetic_percentage / 2, i, f\"{non_diabetic_percentage:.1f}%\", va='center', ha='center', fontsize=12, color='white', fontweight='bold')\n",
    "    if diabetic_percentage > 0:\n",
    "        ax.text(100 - diabetic_percentage / 2, i, f\"{diabetic_percentage:.1f}%\", va='center', ha='center', fontsize=12, color='white', fontweight='bold')\n",
    "\n",
    "# Get rightmost x-limit for alignment\n",
    "x_max = plt.xlim()[1]  \n",
    "text_x = x_max + 5  # Move labels further right\n",
    "\n",
    "# Add Count Labels to Each Age Group\n",
    "for i, (age_group, row) in enumerate(age_group_counts.iterrows()):\n",
    "    non_diabetic_count = row.get(0, 0)  # Get count safely\n",
    "    diabetic_count = row.get(1, 0)  # Get count safely\n",
    "\n",
    "    plt.text(text_x, i + 0.15, f\"Non-Diabetic: {non_diabetic_count}\", fontsize=13,\n",
    "             fontweight=\"bold\", color=non_diabetic_color, ha=\"left\")\n",
    "\n",
    "    plt.text(text_x, i - 0.15, f\"Diabetic: {diabetic_count}\", fontsize=13,\n",
    "             fontweight=\"bold\", color=diabetic_color, ha=\"left\")\n",
    "\n",
    "# Improve plot readability\n",
    "plt.title(\"Age Distribution Across Different Age Groups\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Percentage\", fontsize=14)  # Changed to \"Percentage\" to reflect the x-axis data\n",
    "plt.ylabel(\"Age Group\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.legend([\"Negative\", \"Positive\"], title=\"Diabetic Status\", title_fontsize=\"13\", fontsize=\"12\", loc=\"upper right\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2.3 Drop the 'age_group' column\n",
    "pima_df.drop(columns=['age_group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.3 Categorical Feature Analysis\n",
    "# ----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 15))\n",
    "\n",
    "# Extract colors from the 'Set2' palette\n",
    "palette = sns.color_palette('Set2')\n",
    "negative_color = palette[0]\n",
    "positive_color = palette[1]\n",
    "\n",
    "# ----------------------------\n",
    "# Plot glucose level grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[0, 0], x='Glucose', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[0, 0].set_title('Glucose Level grouped by Outcome', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Glucose', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[0, 0].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot blood pressure grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[0, 1], x='BloodPressure', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[0, 1].set_title('Blood Pressure grouped by Outcome', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Blood Pressure', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot BMI grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[1, 0], x='BMI', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[1, 0].set_title('BMI grouped by Outcome', fontsize=14)\n",
    "axes[1, 0].set_xlabel('BMI', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[1, 0].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot insulin level grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[1, 1], x='Insulin', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[1, 1].set_title('Insulin Level grouped by Outcome', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Insulin', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot skin thickness grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[2, 0], x='SkinThickness', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[2, 0].set_title('Skin Thickness grouped by Outcome', fontsize=14)\n",
    "axes[2, 0].set_xlabel('Skin Thickness', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[2, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[2, 0].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot Pregnancies grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[2, 1], x='Pregnancies', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[2, 1].set_title('Pregnancies grouped by Outcome', fontsize=14)\n",
    "axes[2, 1].set_xlabel('Pregnancies', fontsize=12)\n",
    "axes[2, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[2, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[2, 1].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot Diabetes Pedigree Function grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[3, 0], x='DiabetesPedigreeFunction', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[3, 0].set_title('Diabetes Pedigree Function(DPF) grouped by Outcome', fontsize=14)\n",
    "axes[3, 0].set_xlabel('Diabetes Pedigree Function(DPF)', fontsize=12)\n",
    "axes[3, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[3, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[3, 0].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plot Age grouped by outcome\n",
    "# ----------------------------\n",
    "sns.histplot(\n",
    "    ax=axes[3, 1], x='Age', hue='Outcome', data=pima_df, kde=True, bins=20, palette='Set2'\n",
    ")\n",
    "axes[3, 1].set_title('Age grouped by Outcome', fontsize=14)\n",
    "axes[3, 1].set_xlabel('Age', fontsize=12)\n",
    "axes[3, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[3, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "axes[3, 1].legend(title='Outcome', labels=['Positive', 'Negative'], loc='upper right')\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Adjust the layout\n",
    "# ----------------------------\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4.4 Categorical Feature Analysis\n",
    "# ----------------------------\n",
    "#4.4.1 Heatmap of correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pima_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Features', fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.2 Top correlated features with Class\n",
    "correlations = pima_df.corr()['Outcome'].drop('Outcome').sort_values(ascending=False)\n",
    "print(\"Top Correlated Features with Class:\")\n",
    "print(correlations)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create bar plot\n",
    "sns.barplot(x=correlations.values, y=correlations.index, dodge=False, palette='Set2')\n",
    "\n",
    "# Add count labels on each bar\n",
    "for i, v in enumerate(correlations):\n",
    "    plt.text(v, i, f\"{v:.2f}\", color='black', va='center', fontsize=12)\n",
    "\n",
    "# Improve readability\n",
    "plt.xlabel(\"Correlation Coefficient\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title('Feature Correlations with Outcome', fontsize=16, fontweight=\"bold\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 5: Feature Engineering------------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 5.1 Data Splitting\n",
    "# ----------------------------\n",
    "#5.1.1 Split the dataset into features (X) and target variable (y)\n",
    "X = pima_df.drop(columns=['Outcome'])\n",
    "y = pima_df['Outcome']\n",
    "\n",
    "# Display the shapes of X and y to verify the split\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1.2 Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training Features shape:\", X_train.shape)\n",
    "print(\"Testing Features shape:\", X_test.shape)\n",
    "print(\"Training Target shape:\", y_train.shape)\n",
    "print(\"Testing Target shape:\", y_test.shape)\n",
    "\n",
    "print(\"Training class distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Testing class distribution:\\n\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5.2 Data SMOTE\n",
    "# ----------------------------\n",
    "#5.3.1 Apply SMOTE to balance the target variable\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the shapes of the training set after SMOTE\n",
    "print(\"Training Features shape after SMOTE:\", X_train.shape)\n",
    "print(\"Training Target shape after SMOTE:\", y_train.shape)\n",
    "\n",
    "print(\"Training class distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Testing class distribution:\\n\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5.3 Data Scaling\n",
    "# ----------------------------\n",
    "# Create StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# Display the shape of the splits\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 6: Model Building and Hyperparameter Tuning-----------\n",
    "# ========================================================================\n",
    "# ----------------------------\n",
    "# 6.1 Naive Bayes\n",
    "# ----------------------------\n",
    "# 6.1.1 Model Building and Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Initialize the RandomizedSearchCV object\n",
    "nb_search = GridSearchCV(\n",
    "    estimator=nb, \n",
    "    param_grid={\"var_smoothing\": np.logspace(0,-2, num=100)},  # Use GridSearchCV \n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=1  # Display progress\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "nb_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "nb_best_params = nb_search.best_params_\n",
    "print(\"Best Parameters for Naive Bayes:\", nb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.1.2 Model Evaluation\n",
    "\n",
    "# Initialize the Naive Bayes classifier with the best parameters\n",
    "nb_best_model = GaussianNB(var_smoothing=nb_best_params['var_smoothing'])\n",
    "nb_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_nb = nb_best_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "nb_acc = accuracy_score(y_test, y_pred_nb)\n",
    "nb_conf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
    "nb_roc_auc = roc_auc_score(y_test, y_pred_nb)\n",
    "nb_f1 = f1_score(y_test, y_pred_nb)\n",
    "nb_precision = precision_score(y_test, y_pred_nb)\n",
    "nb_recall = recall_score(y_test, y_pred_nb)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Naive Bayes Accuracy: {nb_acc:.2f}\")\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(nb_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "print(f\"ROC-AUC Score: {nb_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {nb_f1:.2f}\")\n",
    "print(f\"Precision: {nb_precision:.2f}\")\n",
    "print(f\"Recall: {nb_recall:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(nb_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.2 Logistic Regression\n",
    "# ----------------------------\n",
    "# 6.2.1 Model Building and Hyperparameter Tuning using GridSearchCV\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "lr_params = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # Regularization strengths\n",
    "    \"penalty\": [\"l2\", \"l1\"],  # Regularization penalties\n",
    "    \"solver\": [\"liblinear\", \"saga\"],  # Solvers compatible with both l1 and l2 penalties\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=lr_params,\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    verbose=1,  # Display progress during fitting\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model with hyperparameter tuning\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "lr_best_params = lr_grid.best_params_\n",
    "print(\"Best Parameters for Logistic Regression:\", lr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2.2 Model Evaluation\n",
    "\n",
    "# Initialize the Logistic Regression classifier with the best parameters\n",
    "lr_best_model = LogisticRegression(**lr_best_params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_lr = lr_best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "lr_roc_auc = roc_auc_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Logistic Regression Accuracy: {lr_acc:.2f}\")\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(lr_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(f\"ROC-AUC Score: {lr_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {lr_f1:.2f}\")\n",
    "print(f\"Precision: {lr_precision:.2f}\")\n",
    "print(f\"Recall: {lr_recall:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(lr_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.3 Random Forest\n",
    "# ----------------------------\n",
    "\n",
    "# 6.3.1 Model Building and Hyperparameter Tuning\n",
    "\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier()  # Set random_state for reproducibility\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],  # Number of trees in the forest\n",
    "    \"criterion\": [\"gini\", \"entropy\"],  # Split criterion\n",
    "    \"max_depth\": [10, 20, 30, 40, 50, None],  # Maximum depth of the tree\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split an internal node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_params,\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    verbose=1,  # Display progress\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to find the best hyperparameters\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "rf_best_params = rf_grid.best_params_\n",
    "print(\"Best Parameters for Random Forest:\", rf_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6.3.2 Model Evaluation\n",
    "\n",
    "# Initialize the Random Forest classifier with the best hyperparameters\n",
    "rf_best_model = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "# Fit the final model on the training data\n",
    "rf_best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict the outcomes for the test set\n",
    "rf_pred = rf_best_model.predict(X_test)\n",
    "rf_pred_prob = rf_best_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_pred)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_pred_prob)\n",
    "rf_f1 = f1_score(y_test, rf_pred)\n",
    "rf_precision = precision_score(y_test, rf_pred)\n",
    "rf_recall = recall_score(y_test, rf_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.3f}\")\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(rf_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "print(f\"Random Forest ROC-AUC Score: {rf_roc_auc:.3f}\")\n",
    "print(f\"F1-Score: {rf_f1:.3f}\")\n",
    "print(f\"Precision: {rf_precision:.3f}\")\n",
    "print(f\"Recall: {rf_recall:.3f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.4 Support Vector Machine (SVM)\n",
    "# ----------------------------\n",
    "\n",
    "# 6.4.1 Model Building and Hyperparameter Tuning\n",
    "\n",
    "# Initialize the Support Vector Machine classifier\n",
    "\n",
    "#svm = SVC()  # Set random_state for reproducibility\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "#svm_params = {\n",
    " #\n",
    "#   \"C\": [1, 10],  # Regularization parameter\n",
    " #\n",
    "#   \"kernel\": [\"linear\", \"rbf\"],  # Kernel type\n",
    "#\n",
    "#    \"degree\": [2, 3, 4],  # Degree for the polynomial kernel\n",
    " #\n",
    "#   \"gamma\": [\"scale\", \"auto\"],  # Kernel coefficient\n",
    "#\n",
    "#}\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "#\n",
    "#svm_grid = GridSearchCV(\n",
    " #\n",
    "#   estimator=svm,\n",
    "#\n",
    "#    param_grid=svm_params,\n",
    "  #\n",
    "#  scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "  #\n",
    "#  cv=10,  # 10-fold cross-validation\n",
    "  #\n",
    "#  verbose=1,  # Display progress\n",
    " #\n",
    "#   n_jobs=-1  # Use all available processors\n",
    "#\n",
    "#)\n",
    "\n",
    "# Fit the GridSearchCV object to find the best hyperparameters\n",
    "#\n",
    "##\n",
    "#svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "#\n",
    "#svm_best_params = svm_grid.best_params_\n",
    "#\n",
    "#print(\"Best Parameters for Support Vector Machine:\", svm_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4.2 Model Evaluation\n",
    "\n",
    "# Initialize the SVM classifier with the best hyperparameters\n",
    "#svm_best_model = SVC(**svm_best_params)\n",
    "\n",
    "# Fit the final model on the training data\n",
    "#svm_best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the outcomes for the test set\n",
    "#svm_pred = svm_best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "#svm_acc = accuracy_score(y_test, svm_pred)\n",
    "#svm_conf_matrix = confusion_matrix(y_test, svm_pred)\n",
    "#svm_roc_auc = roc_auc_score(y_test, svm_pred)\n",
    "#svm_f1 = f1_score(y_test, svm_pred)\n",
    "#svm_precision = precision_score(y_test, svm_pred)\n",
    "#svm_recall = recall_score(y_test, svm_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "#print(f\"SVM Accuracy: {svm_acc:.3f}\")\n",
    "#print(\"SVM Confusion Matrix:\")\n",
    "#print(svm_conf_matrix)\n",
    "#print(\"\\nClassification Report:\\n\", classification_report(y_test, svm_pred))\n",
    "#print(f\"SVM ROC-AUC Score: {svm_roc_auc:.3f}\")\n",
    "#print(f\"F1-Score: {svm_f1:.3f}\")\n",
    "#print(f\"Precision: {svm_precision:.3f}\")\n",
    "#print(f\"Recall: {svm_recall:.3f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "#plt.figure(figsize=(6, 4))\n",
    "#sns.heatmap(svm_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "#plt.title(\"SVM Confusion Matrix\")\n",
    "#plt.xlabel(\"Predicted\")\n",
    "#plt.ylabel(\"Actual\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.5 Extreme Gradient Boosting (XGBoost)\n",
    "# ----------------------------\n",
    "\n",
    "# 6.5.1 Model Building and Hyperparameter Tuning\n",
    "\n",
    "# Initialize the Gradient Boosting (XGBoost) classifier\n",
    "gb = XGBClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.5],  # Step size shrinkage\n",
    "    'max_depth': [2, 3, 4, 5],  # Maximum depth of trees\n",
    "    'subsample': [0.5, 0.8, 1.0],  # Fraction of samples for training\n",
    "    'colsample_bytree': [0.1, 0.2, 0.3, 0.5],  # Fraction of features per tree\n",
    "    'colsample_bylevel': [0.1, 0.2, 0.3, 0.5],  # Fraction of features per level\n",
    "    'min_child_weight': [1, 3, 5, 7],  # Minimum sum of weights of child nodes\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "gb_grid = GridSearchCV(\n",
    "    estimator=gb,\n",
    "    param_grid=gb_params,\n",
    "    scoring=\"roc_auc\",  # Optimize for ROC-AUC\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to find the best hyperparameters\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters and best estimator\n",
    "gb_best_params = gb_grid.best_params_\n",
    "gb_model = gb_grid.best_estimator_\n",
    "print(\"Best Parameters for Gradient Boosting (XGBoost):\", gb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6.5.2 Model Evaluation\n",
    "\n",
    "# Initialize the Gradient Boosting (XGBoost) classifier with the best hyperparameters\n",
    "gb_best_model = XGBClassifier(**gb_best_params)\n",
    "\n",
    "# Fit the final model on the training data\n",
    "gb_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the test set\n",
    "gb_pred = gb_best_model.predict(X_test)\n",
    "gb_pred_prob = gb_best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "gb_acc = accuracy_score(y_test, gb_pred)\n",
    "gb_conf_matrix = confusion_matrix(y_test, gb_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, gb_pred_prob)\n",
    "gb_f1 = f1_score(y_test, gb_pred)\n",
    "gb_precision = precision_score(y_test, gb_pred)\n",
    "gb_recall = recall_score(y_test, gb_pred)\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "sensitivity = gb_conf_matrix[1, 1] / (gb_conf_matrix[1, 1] + gb_conf_matrix[1, 0])\n",
    "specificity = gb_conf_matrix[0, 0] / (gb_conf_matrix[0, 0] + gb_conf_matrix[0, 1])\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Gradient Boosting Accuracy: {gb_acc:.2f}\")\n",
    "print(\"Gradient Boosting Confusion Matrix:\")\n",
    "print(gb_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, gb_pred))\n",
    "print(f\"ROC-AUC Score: {gb_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {gb_f1:.2f}\")\n",
    "print(f\"Precision: {gb_precision:.2f}\")\n",
    "print(f\"Recall: {gb_recall:.2f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(gb_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Gradient Boosting Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6.6 Stacking Classifier\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize the base classifiers\n",
    "base_classifiers = [\n",
    "    LogisticRegression(**lr_best_params),\n",
    "    RandomForestClassifier(**rf_best_params),\n",
    "    XGBClassifier(**gb_best_params)\n",
    "]\n",
    "\n",
    "# Define the meta-classifier\n",
    "meta_classifier = RandomForestClassifier()\n",
    "\n",
    "# Initialize the StackingCV classifier\n",
    "stacking = StackingCVClassifier(\n",
    "    classifiers=base_classifiers,\n",
    "    meta_classifier=meta_classifier,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    stratify=True,  # Use stratified folds\n",
    "    shuffle=True,  # Shuffle the data\n",
    "    n_jobs=-1,  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the StackingCV classifier\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable\n",
    "y_stacking_pred = stacking.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "stacking_acc = accuracy_score(y_test, y_stacking_pred)\n",
    "stacking_conf_matrix = confusion_matrix(y_test, y_stacking_pred)\n",
    "stacking_roc_auc = roc_auc_score(y_test, y_stacking_pred)\n",
    "stacking_f1 = f1_score(y_test, y_stacking_pred)\n",
    "stacking_precision = precision_score(y_test, y_stacking_pred)\n",
    "stacking_recall = recall_score(y_test, y_stacking_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Stacking Classifier Accuracy: {stacking_acc:.2f}\")\n",
    "print(\"Stacking Classifier Confusion Matrix:\")\n",
    "print(stacking_conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_stacking_pred))\n",
    "print(f\"ROC-AUC Score: {stacking_roc_auc:.2f}\")\n",
    "print(f\"F1-Score: {stacking_f1:.2f}\")\n",
    "print(f\"Precision: {stacking_precision:.2f}\")\n",
    "print(f\"Recall: {stacking_recall:.2f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(stacking_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.title(\"Stacking Classifier Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 7: Model Performance Comparison------------\n",
    "# ========================================================================\n",
    "\n",
    "# 7.1 Model Performance Metrics \n",
    "\n",
    "# Create a DataFrame to display the model performance metrics\n",
    "\n",
    "# Initialize data for the model performance DataFrame\n",
    "data = {\n",
    "    \"Model\": [\"NB\", \"LR\", \"RF\", \"XGB\", \"Stacking\"],\n",
    "    \"Accuracy\": [round(nb_acc, 3), round(lr_acc, 3), round(rf_acc, 3), round(gb_acc, 3), round(stacking_acc, 3)],\n",
    "    \"ROC-AUC\": [round(nb_roc_auc, 3), round(lr_roc_auc, 3), round(rf_roc_auc, 3), round(gb_roc_auc, 3), round(stacking_roc_auc, 3)],\n",
    "    \"F1-Score\": [round(nb_f1, 3), round(lr_f1, 3), round(rf_f1, 3), round(gb_f1, 3), round(stacking_f1, 3)],\n",
    "    \"Precision\": [round(nb_precision, 3), round(lr_precision, 3), round(rf_precision, 3), round(gb_precision, 3), round(stacking_precision, 3)],\n",
    "    \"Recall\": [round(nb_recall, 3), round(lr_recall, 3), round(rf_recall, 3), round(gb_recall, 3), round(stacking_recall, 3)]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "model_comparison = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "\n",
    "# 7.2 Visualizing Model  Performance\n",
    "\n",
    "\n",
    "# 7.2.1 Create a Bar Plot for Model Comparison\n",
    "\n",
    "# Convert the data to long format\n",
    "model_comparison_long = pd.melt(model_comparison, id_vars=[\"Model\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# customed color\n",
    "colors = [\"#e2edc9\", \"#a4d3b7\", \"#65c4b9\", \"#26a7c8\", \"#607bbc\"]\n",
    "# Create a bar plot to compare the model performance metrics\n",
    "sns.barplot(x=\"Model\", y=\"Value\", hue=\"Metric\", data=model_comparison_long, palette=colors)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Model Performance Comparison\", fontsize=16)\n",
    "plt.xlabel(\"Model\", fontsize=16)\n",
    "plt.ylabel(\"Performance Metric\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=12, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ----------Section 8: Feature Interpretability------------\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8.1 Importance ranking of features in the model built-in\n",
    "# ----------------------------\n",
    "# 8.1.1 Random Forest Feature Importance\n",
    "# Extract feature importances from the Random Forest model\n",
    "rf_feature_importance = rf_best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importance values\n",
    "rf_fi_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": rf_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the DataFrame based on feature importance\n",
    "rf_fi_df = rf_fi_df.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the feature importances\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(rf_fi_df)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=rf_fi_df, palette=\"viridis\")\n",
    "\n",
    "# Add percentage labels on each bar\n",
    "for index, value in enumerate(rf_fi_df[\"Importance\"]):\n",
    "    plt.text(value, index, f'{value:.2%}', color='black', va=\"center\", fontsize=8)\n",
    "\n",
    "plt.title(\"Random Forest Feature Importance\", fontsize=16)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2 Gradient Boosting Feature Importance\n",
    "\n",
    "# Extract feature importances from the Gradient Boosting model\n",
    "gb_feature_importance = gb_best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importance values\n",
    "gb_fi_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": gb_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the DataFrame based on feature importance\n",
    "gb_fi_df = gb_fi_df.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the feature importances\n",
    "print(\"\\nGradient Boosting Feature Importance:\")\n",
    "print(gb_fi_df)\n",
    "\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=gb_fi_df, palette=\"viridis\")\n",
    "\n",
    "# Add percentage labels on each bar\n",
    "for index, value in enumerate(gb_fi_df[\"Importance\"]):\n",
    "    plt.text(value, index, f'{value:.2%}', color='black', va=\"center\", fontsize=6)\n",
    "\n",
    "plt.title(\"Gradient Boosting Feature Importance\", fontsize=16)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8.2 Importance ranking of features by SHAP values\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# 8.2.1 SHAP Analysis for Random Forest\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize the SHAP explainer for the best Random Forest model\n",
    "rf_explainer = shap.Explainer(rf_best_model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "rf_shap_values = rf_explainer.shap_values(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Visualize the SHAP values\n",
    "shap.summary_plot(rf_shap_values[:,:,1], X_test,  feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.2.2 SHAP Analysis for Gradient Boosting\n",
    "# Create object that can calculate shap values for Gradient Boosting model\n",
    "explainer_gb = shap.TreeExplainer(gb_best_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of X_test rather than a single row, to have more data for plot.\n",
    "shap_values_gb = explainer_gb.shap_values(X_test)\n",
    "\n",
    "# Make plot for Gradient Boosting model\n",
    "shap.summary_plot(shap_values_gb, X_test, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8.3 Local Interpretability using LIME\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# 8.3.1 True Positive Case in PIMA\n",
    "# ----------------------------\n",
    "# Select a true positive case from the test set\n",
    "true_positive = y_test[(y_test == 1) & (rf_pred == 1) & (gb_pred ==1) & (y_stacking_pred == 1)].index[0]\n",
    "\n",
    "\n",
    "# Extract the features for the true positive case\n",
    "PIMA_X_true_positive = X_test.loc[[true_positive]]\n",
    "\n",
    "print(\"\\nTrue Positive Case:\")\n",
    "print(PIMA_X_true_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true positive case via random forest\n",
    "\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true positive case\n",
    "exp = explainer.explain_instance(PIMA_X_true_positive.values[0], rf_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true positive case via XGBoost\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true positive case\n",
    "exp = explainer.explain_instance(PIMA_X_true_positive.values[0], gb_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true positive case via stacking classifier\n",
    "\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true positive case\n",
    "exp = explainer.explain_instance(PIMA_X_true_positive.values[0], stacking.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8.3.2 True Negative Case in PIMA\n",
    "# ----------------------------\n",
    "# Select a true negative case from the test set\n",
    "true_negative = y_test[(y_test == 0) & (rf_pred == 0) & (gb_pred ==0) & (y_stacking_pred == 0)].index[0]\n",
    "\n",
    "# Extract the features for the true negative case\n",
    "PIMA_X_true_negative = X_test.loc[[true_negative]]\n",
    "print(\"\\nTrue Negative Case:\")\n",
    "print(PIMA_X_true_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lime explainer for the true negative case via random forest\n",
    "\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "# Explain the true negative case\n",
    "exp = explainer.explain_instance(PIMA_X_true_negative.values[0], rf_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true negative case via XGBoost\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "\n",
    "# Explain the true negative case\n",
    "exp = explainer.explain_instance(PIMA_X_true_negative.values[0], gb_best_model.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime explainer for the true negative case via stacking classifier\n",
    "\n",
    "# Initialize the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", feature_names=X.columns, class_names=[\"Negative\", \"Positive\"])\n",
    "\n",
    "# Explain the true negative case\n",
    "exp = explainer.explain_instance(PIMA_X_true_negative.values[0], stacking.predict_proba, num_features=16, top_labels=1)\n",
    "\n",
    "# Display the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
